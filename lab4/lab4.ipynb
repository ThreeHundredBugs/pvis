{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/04 00:12:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('lab4').getOrCreate()\n",
    "df = spark.read.csv('DS_2019_public.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "@udf(IntegerType())\n",
    "def climate_code_to_binary(code: Literal[1, 2, 3, 4, 5]) -> Literal[0, 1]:\n",
    "    match code:\n",
    "        case 3:\n",
    "            return 0\n",
    "        case 1 | 2 | 4 | 5:\n",
    "            return 1\n",
    "\n",
    "@udf(StringType())\n",
    "def climate_code_to_climate_name(code: Literal[1, 2, 3, 4, 5]) -> str:\n",
    "    match code:\n",
    "        case 1:\n",
    "            return 'Very cold / Cold'\n",
    "        case 2:\n",
    "            return 'Hot dry / Mixed dry'\n",
    "        case 3:\n",
    "            return 'Hot humid'\n",
    "        case 4:\n",
    "            return 'Mixed humid'\n",
    "        case 5:\n",
    "            return 'Marine'\n",
    "    return ''\n",
    "\n",
    "df = df.withColumn(\n",
    "    'Climate',\n",
    "    climate_code_to_climate_name(df['Climate_Region_Pub']),\n",
    ").withColumn(\n",
    "    'Class',\n",
    "    climate_code_to_binary(df['Climate_Region_Pub'])\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features: ['TEMPHOME', 'TEMPGONE', 'KWHSPH', 'KWHCOL', 'CUFEETNGSPH', 'GALLONFOSPH']\n"
     ]
    }
   ],
   "source": [
    "explain = {\n",
    "    'Climate': 'Climate',\n",
    "    'Class': 'Class',\n",
    "    'TEMPHOME':\t'Temperature when someone is home during the day (winter)',\n",
    "    'TEMPGONE':\t'Temperature when no on is home during the day (winter)',\n",
    "    'KWHSPH': 'Electricity usage for space heating, in kilowatt-hours, 2009',\n",
    "    'KWHCOL': 'Electricity usage for air-conditioning, central and window/wall (room), in kilowatt-hours, 2009',\n",
    "    'CUFEETNGSPH': 'Natural Gas usage for space heating, in hundred cubic feet, 2009',\n",
    "    'GALLONFOSPH': 'LPG/Propane usage for space heating, in gallons, 2009',\n",
    "}\n",
    "\n",
    "input_features = list(explain.keys())\n",
    "df = df.select(input_features)\n",
    "input_features.remove('Climate')\n",
    "input_features.remove('Class')\n",
    "print(f'Input features: {input_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+-------+-------+-----------+-----------+\n",
      "|Class|TEMPHOME|TEMPGONE| KWHSPH| KWHCOL|CUFEETNGSPH|GALLONFOSPH|\n",
      "+-----+--------+--------+-------+-------+-----------+-----------+\n",
      "|    1|   67.47|   64.21|  991.4|1124.72|     291.16|      45.81|\n",
      "|    0|   65.42|    62.7|1071.41|4225.86|      54.69|       0.22|\n",
      "+-----+--------+--------+-------+-------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "grouped = df.groupBy('Class').mean()\n",
    "grouped.select('Class', *[round(f'avg({c})', 2).alias(c) for c in input_features]).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Climate: string (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- TEMPHOME: integer (nullable = true)\n",
      " |-- TEMPGONE: integer (nullable = true)\n",
      " |-- KWHSPH: double (nullable = true)\n",
      " |-- KWHCOL: double (nullable = true)\n",
      " |-- CUFEETNGSPH: double (nullable = true)\n",
      " |-- GALLONFOSPH: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(\n",
    "    inputCols=input_features,\n",
    "    outputCol='features',\n",
    "\n",
    ")\n",
    "df = df_assembler.transform(df)\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|            Climate|              Class|          TEMPHOME|          TEMPGONE|            KWHSPH|            KWHCOL|       CUFEETNGSPH|       GALLONFOSPH|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|              10875|              10875|             10875|             10875|             10875|             10875|             10875|             10875|\n",
      "|   mean|               null| 0.8200459770114943|  67.0968275862069|63.940781609195405|1005.7949160459785|1682.7826961839055|248.61066510344867| 37.60788570114944|\n",
      "| stddev|               null|0.38416681735215996|13.589724722693937|13.719448689129953|1591.1841533271422| 2480.831034235845|334.31380003202287|154.43127656769101|\n",
      "|    min|Hot dry / Mixed dry|                  0|                 2|                 2|               0.0|               0.0|               0.0|               0.0|\n",
      "|    max|   Very cold / Cold|                  1|                90|                90|         13843.341|         60995.431|          3296.035|          2407.218|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|            Climate|count|\n",
      "+-------------------+-----+\n",
      "|             Marine|  612|\n",
      "|        Mixed humid| 3169|\n",
      "|   Very cold / Cold| 3593|\n",
      "|Hot dry / Mixed dry| 1544|\n",
      "|          Hot humid| 1957|\n",
      "+-------------------+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1| 8918|\n",
      "|    0| 1957|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Climate').count().show()\n",
    "df.groupBy('Class').count().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records for training: 7588\n",
      "root\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+-------------------+\n",
      "|Class|           features|\n",
      "+-----+-------------------+\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1| 6237|\n",
      "|    0| 1351|\n",
      "+-----+-----+\n",
      "\n",
      "Records for testing: 3287\n",
      "root\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+-------------------+\n",
      "|Class|           features|\n",
      "+-----+-------------------+\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "|    0|(6,[0,1],[2.0,2.0])|\n",
      "+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1| 2681|\n",
      "|    0|  606|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df.select('Class', 'features').randomSplit([0.7, 0.3])\n",
    "print(f'Records for training: {df_train.count()}')\n",
    "df_train.printSchema()\n",
    "df_train.show(5)\n",
    "df_train.groupBy('Class').count().show(5)\n",
    "\n",
    "print(f'Records for testing: {df_test.count()}')\n",
    "df_test.printSchema()\n",
    "df_test.show(5)\n",
    "df_test.groupBy('Class').count().show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/04 00:12:56 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/01/04 00:12:56 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/01/04 00:12:56 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logistic_reg = LogisticRegression(labelCol='Class').fit(df_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "Total records: 7588\n",
      "Linear regression true positives: 5994\n",
      "Linear regression true negatives: 661\n",
      "Linear regression false positives: 690\n",
      "Linear regression false negatives: 243\n"
     ]
    }
   ],
   "source": [
    "train_results = logistic_reg.evaluate(df_train).predictions\n",
    "\n",
    "TP = train_results[\n",
    "    (train_results['Class'] == 1) & (train_results['prediction'] == 1)\n",
    "].count()\n",
    "TN = train_results[\n",
    "    (train_results['Class'] == 0) & (train_results['prediction'] == 0)\n",
    "].count()\n",
    "FP = train_results[\n",
    "    (train_results['Class'] == 0) & (train_results['prediction'] == 1)\n",
    "].count()\n",
    "FN = train_results[\n",
    "    (train_results['Class'] == 1) & (train_results['prediction'] == 0)\n",
    "].count()\n",
    "\n",
    "print('Train results')\n",
    "print(f'Total records: {df_train.count()}')\n",
    "print(f'Linear regression true positives: {TP}')\n",
    "print(f'Linear regression true negatives: {TN}')\n",
    "print(f'Linear regression false positives: {FP}')\n",
    "print(f'Linear regression false negatives: {FN}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results\n",
      "Total records: 3287\n",
      "True positives: 2568\n",
      "True negatives: 289\n",
      "False positives: 317\n",
      "False negatives: 113\n",
      "Accuracy: 0.8691816245816855\n",
      "Recall: 0.9578515479298769\n",
      "Precision: 0.8901213171577123\n"
     ]
    }
   ],
   "source": [
    "test_results = logistic_reg.evaluate(df_test).predictions\n",
    "\n",
    "TP = test_results[\n",
    "    (test_results['Class'] == 1) & (test_results['prediction'] == 1)\n",
    "].count()\n",
    "TN = test_results[\n",
    "    (test_results['Class'] == 0) & (test_results['prediction'] == 0)\n",
    "].count()\n",
    "FP = test_results[\n",
    "    (test_results['Class'] == 0) & (test_results['prediction'] == 1)\n",
    "].count()\n",
    "FN = test_results[\n",
    "    (test_results['Class'] == 1) & (test_results['prediction'] == 0)\n",
    "].count()\n",
    "\n",
    "print('Test results')\n",
    "print(f'Total records: {df_test.count()}')\n",
    "print(f'True positives: {TP}')\n",
    "print(f'True negatives: {TN}')\n",
    "print(f'False positives: {FP}')\n",
    "print(f'False negatives: {FN}')\n",
    "print(f'Accuracy: {(TP + TN) / df_test.count()}')\n",
    "print(f'Recall: {TP / (TP + FN)}')\n",
    "print(f'Precision: {TP / (TP + FP)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|  501|\n",
      "|       1.0| 2786|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol='Class',\n",
    "    numTrees=50,\n",
    ").fit(df_train)\n",
    "rf_predictions = rf_classifier.transform(df_test)\n",
    "rf_predictions.groupBy('prediction').count().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest results:\n",
      "Accuracy: 0.8846972923638576\n",
      "Precision: 0.8787368506737985\n",
      "AUC: 0.9255176077100433\n",
      "Feature importances:\n",
      " TEMPHOME: 0.029860339755578797\n",
      "TEMPGONE: 0.015241238109706479\n",
      "KWHSPH: 0.1381960748225932\n",
      "KWHCOL: 0.5972012350752995\n",
      "CUFEETNGSPH: 0.20737063566603878\n",
      "GALLONFOSPH: 0.012130476570783234\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "rf_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol='Class',\n",
    "    metricName='accuracy',\n",
    ").evaluate(rf_predictions)\n",
    "rf_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol='Class',\n",
    "    metricName='weightedPrecision',\n",
    ").evaluate(rf_predictions)\n",
    "rf_auc = BinaryClassificationEvaluator(labelCol='Class').evaluate(rf_predictions)\n",
    "\n",
    "print('Random forest results:')\n",
    "print(f'Accuracy: {rf_accuracy}')\n",
    "print(f'Precision: {rf_precision}')\n",
    "print(f'AUC: {rf_auc}')\n",
    "\n",
    "_idx_to_name = {\n",
    "    feature['idx']: feature['name']\n",
    "    for feature in df_train.schema['features'].metadata['ml_attr']['attrs']['numeric']\n",
    "}\n",
    "print(\n",
    "    'Feature importances:\\n',\n",
    "    '\\n'.join(\n",
    "        f'{_idx_to_name[idx]}: {value}'\n",
    "        for idx, value in enumerate(rf_classifier.featureImportances.values)\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
